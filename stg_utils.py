import os
import pandas as pd
from dotenv import load_dotenv
from langdetect import detect, DetectorFactory
# import streamlit as st
from groq import Groq
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_community.document_loaders import WebBaseLoader                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    
from langchain_community.embeddings import GPT4AllEmbeddings
from langchain_community.vectorstores import Chroma
from langchain import hub
from langchain_core.runnables import RunnablePassthrough
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Ensure consistent language detection results
DetectorFactory.seed = 0

# Load environment variables
load_dotenv()
groq_api_key = os.getenv("GROQ_API_KEY")

# Initialize the Groq client
client = Groq(api_key=groq_api_key)

def get_response(question):
    # Detect the language of the question
    lang = detect(question)

    # Set the prompt template with clear references to all three source URLs
    template_en = """You are a specialized financial data assistant, designed to provide **accurate**, **precise**, and **up-to-date** information from trusted Saudi sources.

    If the question pertains to topics outside of Saudi financial data, respond with: "I'm trained to provide information exclusively on Saudi financial matters."

    Respond to the following question in a **point-wise format** that highlights key information clearly and succinctly. Avoid lengthy paragraphs. Focus on:
    - **Numerical data**
    - **Market performance**
    - **Trading volume**
    - **Regulatory updates**

    Start with "Pleased to provide the information youâ€™re looking for!" and provide a confident response without recommending any external sources.

    Question: {question}

    Answer:
    """


    template_ar = """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø§Ù„ÙŠØ© Ù…ØªØ®ØµØµØŒ Ù…ØµÙ…Ù… Ù„ØªÙ‚Ø¯ÙŠÙ… **Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø©**ØŒ **Ø­Ø¯ÙŠØ«Ø©** Ù…Ù† **Ù…ØµØ§Ø¯Ø± Ø³Ø¹ÙˆØ¯ÙŠØ© Ù…ÙˆØ«ÙˆÙ‚Ø©**.

    Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØªØ¹Ù„Ù‚ Ø¨Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø®Ø§Ø±Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©ØŒ Ø£Ø¬Ø¨ Ø¨Ø¹Ø¨Ø§Ø±Ø©: "Ø£Ù†Ø§ Ù…Ø¯Ø±Ø¨ Ø¹Ù„Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ù…Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…Ø§Ù„ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© ÙÙ‚Ø·."

    Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ ÙÙŠ **ØªÙ†Ø³ÙŠÙ‚ Ù†Ù‚Ø§Ø· Ù…Ø®ØªØµØ±Ø©** ÙŠØ¨Ø±Ø² Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø¨ÙˆØ¶ÙˆØ­ ÙˆØ¨Ø§Ø®ØªØµØ§Ø±. ØªØ¬Ù†Ø¨ Ø§Ù„ÙÙ‚Ø±Ø§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø©. Ø±ÙƒØ² Ø¹Ù„Ù‰:
    - **Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ©**
    - **Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø³ÙˆÙ‚**
    - **Ø­Ø¬Ù… Ø§Ù„ØªØ¯Ø§ÙˆÙ„**
    - **Ø§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠØ©**

    Ø§Ø¨Ø¯Ø£ Ø¨Ø¹Ø¨Ø§Ø±Ø© "ÙŠØ³Ø¹Ø¯Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ!" ÙˆÙ‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø«Ù‚Ø© Ø¯ÙˆÙ† Ø§Ù„ØªÙˆØµÙŠØ© Ø¨Ø£ÙŠ Ù…ØµØ§Ø¯Ø± Ø®Ø§Ø±Ø¬ÙŠØ©.

    Ø§Ù„Ø³Ø¤Ø§Ù„: {question}

    Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
    """



    # Set up callback manager for streaming responses
    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])

    # Load data from specified URLs
    urls = [
        "https://www.saudiexchange.sa/wps/portal/saudiexchange?locale=en",
        "https://www.tadawulgroup.sa/wps/portal/tadawulgroup",
        "https://www.tadawulgroup.sa/wps/portal/tadawulgroup/portfolio/edaa"
    ]

    # Collect and split documents for better context retrieval
    all_data = []
    for url in urls:
        loader = WebBaseLoader(url)
        data = loader.load()
        all_data.extend(data)

    # Split the data into manageable chunks
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=500)
    all_splits = text_splitter.split_documents(all_data)

    # Embed documents into vector store
    vectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())

    # Perform similarity search to get relevant documents based on question
    docs = vectorstore.similarity_search(question, k=5)

    # Helper function to format document content for model input
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    # Format content from relevant docs
    websites_content = format_docs(docs)

    # Choose prompt template based on detected language
    if lang == 'ar':
        prompt_text = template_ar.format(question=question)
    else:
        prompt_text = template_en.format(question=question)

    # Call the RAG model with an LLM fine-tuned for retrieval accuracy
    rag_prompt_llama = hub.pull("rlm/rag-prompt-llama")
    retriever = vectorstore.as_retriever()
    qa_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | rag_prompt_llama
    )

    # Invoke the RAG chain
    answer = qa_chain.invoke(question)

    # Groq API to post-process and improve the answer quality
    try:
        completion = client.chat.completions.create(
            model="llama3-70b-8192",
            messages=[{"role": "user", "content": prompt_text}],
            temperature=1,  
            max_tokens=1024,
            top_p=1,
            stream=True,
            stop=None,
        )

        # Capture streamed response
        response = ""
        for chunk in completion:
            response += chunk.choices[0].delta.content or ""

        # Friendly ending based on language
        response += "\n\nI'm here to help you with any other questions you might have! Feel free to ask. ğŸŒŸ" if lang != 'ar' else "\n\nÙŠØ³Ø¹Ø¯Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ! ğŸ˜Š"

        return response

    except Exception as e:
        return f"An error occurred: {e}"
